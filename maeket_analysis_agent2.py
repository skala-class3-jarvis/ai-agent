from dotenv import load_dotenv
load_dotenv()

import os
import json
from typing import Annotated, Sequence, TypedDict, Dict, Any, List, Literal
from pydantic import BaseModel, Field

# LangChain imports
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_openai import ChatOpenAI

# LangGraph imports
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages

# Vector DB (faiss) and document loader imports
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_core.tools.retriever import create_retriever_tool

# External web search tool
from langchain_community.tools import DuckDuckGoSearchRun

# HuggingFace sentence transformers
from langchain_huggingface import HuggingFaceEmbeddings

# =============================================================================
# 1. ë°ì´í„° ë¡œë”© ë° ë²¡í„°ìŠ¤í† ì–´ ì„¸íŒ…
# =============================================================================
DATA_DIR = "data"
PERSIST_PATH = "db_faiss"
EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
    model_name="dragonkue/multilingual-e5-small-ko",
    model_kwargs={"device": "cpu"}
)

def build_vector_db():
    loader = DirectoryLoader(DATA_DIR, glob="**/*.pdf", loader_cls=PyPDFLoader)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs_split = splitter.split_documents(docs)
    db = FAISS.from_documents(docs_split, EMBEDDINGS_MODEL)
    db.save_local(PERSIST_PATH)
    return db

if os.path.exists(PERSIST_PATH):
    db = FAISS.load_local(
        PERSIST_PATH, 
        EMBEDDINGS_MODEL,
        allow_dangerous_deserialization=True
    )
else:
    db = build_vector_db()

retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 5})
search_tool = DuckDuckGoSearchRun()

# =============================================================================
# 2. ê³ ë„í™”ëœ ìƒíƒœ ì •ì˜ (AgentState)
# =============================================================================
class MarketAnalysisState(TypedDict):
    """ì‹œì¥ì„± í‰ê°€ ì—ì´ì „íŠ¸ì˜ ìƒíƒœ"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    query: str  # ì›ë³¸ ì‚¬ìš©ì ì¿¼ë¦¬
    query_type: str  # ì¿¼ë¦¬ ë¶„ë¥˜ ê²°ê³¼ (market_size, trend, competition, forecast ë“±)
    rag_data: str  # ë‚´ë¶€ DB ê²€ìƒ‰ ê²°ê³¼
    web_data: str  # ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ê²°ê³¼
    market_size: str  # ì‹œì¥ ê·œëª¨ ë¶„ì„
    growth_trend: str  # ì„±ì¥ ì¶”ì„¸ ë¶„ì„
    competition: str  # ê²½ìŸ í™˜ê²½ ë¶„ì„
    risk_factors: str  # ë¦¬ìŠ¤í¬ ìš”ì¸ ë¶„ì„
    final_score: int  # ìµœì¢… ì‹œì¥ì„± ì ìˆ˜ (0-100)
    final_report: str  # ìµœì¢… ì¢…í•© ë¦¬í¬íŠ¸
    needs_web_search: bool  # ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€
    analysis_depth: str  # ë¶„ì„ ê¹Šì´ (basic, intermediate, advanced)

# =============================================================================
# 3. LLM ëª¨ë¸ ì •ì˜
# =============================================================================
MODEL_NAME = "gpt-4o-mini"
llm = ChatOpenAI(temperature=0.3, model=MODEL_NAME, streaming=True)
llm_creative = ChatOpenAI(temperature=0.7, model=MODEL_NAME, streaming=True)

# =============================================================================
# 4. ì¿¼ë¦¬ ë¶„ë¥˜ ë° ë¼ìš°íŒ…ì„ ìœ„í•œ Pydantic ëª¨ë¸
# =============================================================================
class QueryClassification(BaseModel):
    """ì¿¼ë¦¬ ë¶„ë¥˜ ê²°ê³¼"""
    query_type: Literal["market_size", "trend", "competition", "forecast", "general"] = Field(
        description="ì¿¼ë¦¬ì˜ ì£¼ìš” ê´€ì‹¬ì‚¬ ë¶„ë¥˜"
    )
    needs_web_search: bool = Field(
        description="ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œì§€ ì—¬ë¶€"
    )
    analysis_depth: Literal["basic", "intermediate", "advanced"] = Field(
        description="í•„ìš”í•œ ë¶„ì„ ê¹Šì´"
    )

# =============================================================================
# 5. ë…¸ë“œ í•¨ìˆ˜ë“¤ ì •ì˜
# =============================================================================

def classify_query_node(state: MarketAnalysisState) -> Dict[str, Any]:
    """
    Step 1: ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ë¶„ë¥˜í•˜ê³  ë¶„ì„ ì „ëµ ê²°ì •
    """
    query = state["query"]
    
    classification_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì‹œì¥ ë¶„ì„ ì¿¼ë¦¬ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ íŒë‹¨í•˜ì„¸ìš”:
1. query_type: ì£¼ìš” ê´€ì‹¬ì‚¬ (market_size, trend, competition, forecast, general)
2. needs_web_search: ìµœì‹  ë°ì´í„°ê°€ í•„ìš”í•œì§€ (true/false)
3. analysis_depth: í•„ìš”í•œ ë¶„ì„ ê¹Šì´ (basic, intermediate, advanced)"""),
        ("human", "{query}")
    ])
    
    structured_llm = llm.with_structured_output(QueryClassification)
    classification = structured_llm.invoke(classification_prompt.format_messages(query=query))
    
    return {
        "query_type": classification.query_type,
        "needs_web_search": classification.needs_web_search,
        "analysis_depth": classification.analysis_depth,
        "messages": [AIMessage(content=f"ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ: {classification.query_type} (ê¹Šì´: {classification.analysis_depth})")]
    }

def retrieve_internal_data_node(state: MarketAnalysisState) -> Dict[str, Any]:
    """
    Step 2: ë‚´ë¶€ ë²¡í„° DBì—ì„œ ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰
    """
    query = state["query"]
    query_type = state.get("query_type", "general")
    
    # ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥
    enhanced_query = f"{query} {query_type} ì‹œì¥ ë¶„ì„"
    
    rag_docs = retriever.invoke(enhanced_query)
    rag_data = "\n\n---\n\n".join([
        f"[ë¬¸ì„œ {i+1}]\n{doc.page_content[:600]}" 
        for i, doc in enumerate(rag_docs)
    ]) if rag_docs else "ê´€ë ¨ ë‚´ë¶€ ë°ì´í„° ì—†ìŒ"
    
    return {
        "rag_data": rag_data[:3000],
        "messages": [AIMessage(content=f"ë‚´ë¶€ DB ê²€ìƒ‰ ì™„ë£Œ: {len(rag_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")]
    }

def web_search_node(state: MarketAnalysisState) -> Dict[str, Any]:
    """
    Step 3: ì™¸ë¶€ ì›¹ ê²€ìƒ‰ (ì¡°ê±´ë¶€ ì‹¤í–‰)
    """
    query = state["query"]
    
    # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
    search_queries = [
        f"{query} ì‹œì¥ ê·œëª¨ ì „ë§ 2024 2025",
        f"{query} ì‚°ì—… ë™í–¥ ë¦¬í¬íŠ¸",
        f"{query} ê²½ìŸì‚¬ ë¶„ì„"
    ]
    
    web_results = []
    for sq in search_queries[:2]:  # ìµœëŒ€ 2ê°œ ì¿¼ë¦¬ë§Œ ì‹¤í–‰
        try:
            result = search_tool.run(sq)
            web_results.append(f"[ê²€ìƒ‰: {sq}]\n{result[:800]}")
        except:
            continue
    
    web_data = "\n\n---\n\n".join(web_results) if web_results else "ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
    
    return {
        "web_data": web_data[:2500],
        "messages": [AIMessage(content="ì›¹ ê²€ìƒ‰ ì™„ë£Œ")]
    }

def analyze_market_size_node(state: MarketAnalysisState) -> Dict[str, Any]:
    """
    Step 4a: ì‹œì¥ ê·œëª¨ ë¶„ì„
    """
    prompt = PromptTemplate(
        input_variables=["query", "rag_data", "web_data"],
        template="""ì‹œì¥ ê·œëª¨ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ, ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œì¥ ê·œëª¨ë¥¼ ë¶„ì„í•˜ì„¸ìš”:

ì¿¼ë¦¬: {query}

ë‚´ë¶€ ë°ì´í„°:
{rag_data}

ì™¸ë¶€ ë°ì´í„°:
{web_data}

ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”:
1. í˜„ì¬ ì‹œì¥ ê·œëª¨ (ê¸ˆì•¡, ë‹¨ìœ„ ëª…ì‹œ)
2. ìµœê·¼ 3-5ë…„ ì„±ì¥ë¥ 
3. ì£¼ìš” ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¹„ì¤‘
4. ì§€ì—­ë³„ ì‹œì¥ ë¶„í¬ (í•´ë‹¹ì‹œ)

ê°„ê²°í•˜ê³  ì •ëŸ‰ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”."""
    )
    
    response = llm.invoke([HumanMessage(
        content=prompt.format(
            query=state["query"],
            rag_data=state.get("rag_data", "N/A"),
            web_data=state.get("web_data", "N/A")
        )
    )])
    
    return {
        "market_size": response.content,
        "messages": [AIMessage(content="ì‹œì¥ ê·œëª¨ ë¶„ì„ ì™„ë£Œ")]
    }

def analyze_growth_trend_node(state: MarketAnalysisState) -> Dict[str, Any]:
    """
    Step 4b: ì„±ì¥ ì¶”ì„¸ ë° íŠ¸ë Œë“œ ë¶„ì„
    """
    prompt = PromptTemplate(
        input_variables=["query", "rag_data", "web_data"],
        template="""ì„±ì¥ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ, ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œì¥ ì„±ì¥ ì¶”ì„¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”:

ì¿¼ë¦¬: {query}

ë‚´ë¶€ ë°ì´í„°:
{rag_data}

ì™¸ë¶€ ë°ì´í„°:
{web_data}

ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”:
1. í–¥í›„ 5ë…„ ì„±ì¥ ì „ë§ (CAGR)
2. ì£¼ìš” ì„±ì¥ ë™ë ¥ (ê¸°ìˆ , ê·œì œ, ì†Œë¹„ì í–‰ë™ ë“±)
3. ì‹œì¥ ì„±ìˆ™ë„ (ë„ì…ê¸°/ì„±ì¥ê¸°/ì„±ìˆ™ê¸°/ì‡ í‡´ê¸°)
4. ìµœì‹  íŠ¸ë Œë“œ ë° í˜ì‹ 

êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê·¼ê±°ë¥¼ í¬í•¨í•˜ì„¸ìš”."""
    )
    
    response = llm.invoke([HumanMessage(
        content=prompt.format(
            query=state["query"],
            rag_data=state.get("rag_data", "N/A"),
            web_data=state.get("web_data", "N/A")
        )
    )])
    
    return {
        "growth_trend": response.content,
        "messages": [AIMessage(content="ì„±ì¥ ì¶”ì„¸ ë¶„ì„ ì™„ë£Œ")]
    }

def analyze_competition_node(state: MarketAnalysisState) -> Dict[str, Any]:
    """
    Step 4c: ê²½ìŸ í™˜ê²½ ë¶„ì„
    """
    prompt = PromptTemplate(
        input_variables=["query", "rag_data", "web_data"],
        template="""ê²½ìŸ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ, ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ìŸ í™˜ê²½ì„ ë¶„ì„í•˜ì„¸ìš”:

ì¿¼ë¦¬: {query}

ë‚´ë¶€ ë°ì´í„°:
{rag_data}

ì™¸ë¶€ ë°ì´í„°:
{web_data}

ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”:
1. ì£¼ìš” ê²½ìŸì‚¬ ë° ì‹œì¥ ì ìœ ìœ¨
2. ì‹œì¥ ì§‘ì¤‘ë„ (HHI ë˜ëŠ” CRn ì§€ìˆ˜ ì–¸ê¸‰ ê°€ëŠ¥ì‹œ)
3. ì§„ì… ì¥ë²½ (ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ) ë° ì´ìœ 
4. ê²½ìŸ ê°•ë„ í‰ê°€

Porterì˜ 5 Forces ê´€ì ì„ í™œìš©í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”."""
    )
    
    response = llm.invoke([HumanMessage(
        content=prompt.format(
            query=state["query"],
            rag_data=state.get("rag_data", "N/A"),
            web_data=state.get("web_data", "N/A")
        )
    )])
    
    return {
        "competition": response.content,
        "messages": [AIMessage(content="ê²½ìŸ í™˜ê²½ ë¶„ì„ ì™„ë£Œ")]
    }

def analyze_risk_factors_node(state: MarketAnalysisState) -> Dict[str, Any]:
    """
    Step 4d: ë¦¬ìŠ¤í¬ ìš”ì¸ ë¶„ì„
    """
    prompt = PromptTemplate(
        input_variables=["query", "market_size", "growth_trend", "competition"],
        template="""ë¦¬ìŠ¤í¬ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ, ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸ì„ ì‹ë³„í•˜ì„¸ìš”:

ì‹œì¥: {query}

ì‹œì¥ ê·œëª¨ ë¶„ì„:
{market_size}

ì„±ì¥ ì¶”ì„¸ ë¶„ì„:
{growth_trend}

ê²½ìŸ í™˜ê²½ ë¶„ì„:
{competition}

ë‹¤ìŒ ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•˜ì„¸ìš”:
1. ì‹œì¥ ë¦¬ìŠ¤í¬ (ìˆ˜ìš” ë³€ë™ì„±, ê²½ê¸° ë¯¼ê°ë„)
2. ê¸°ìˆ  ë¦¬ìŠ¤í¬ (ê¸°ìˆ  ë³€í™”, ëŒ€ì²´ì¬ ìœ„í˜‘)
3. ê·œì œ ë¦¬ìŠ¤í¬ (ë²•ê·œ ë³€í™”, ì •ì±… ë¦¬ìŠ¤í¬)
4. ê²½ìŸ ë¦¬ìŠ¤í¬ (ì‹ ê·œ ì§„ì…ì, ê°€ê²© ê²½ìŸ)

ê° ë¦¬ìŠ¤í¬ë¥¼ ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒìœ¼ë¡œ í‰ê°€í•˜ê³  ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”."""
    )
    
    response = llm.invoke([HumanMessage(
        content=prompt.format(
            query=state["query"],
            market_size=state.get("market_size", "N/A"),
            growth_trend=state.get("growth_trend", "N/A"),
            competition=state.get("competition", "N/A")
        )
    )])
    
    return {
        "risk_factors": response.content,
        "messages": [AIMessage(content="ë¦¬ìŠ¤í¬ ìš”ì¸ ë¶„ì„ ì™„ë£Œ")]
    }

def calculate_market_score_node(state: MarketAnalysisState) -> Dict[str, Any]:
    """
    Step 5: ì¢…í•© ì‹œì¥ì„± ì ìˆ˜ ì‚°ì¶œ
    """
    class MarketScore(BaseModel):
        """ì‹œì¥ì„± ì ìˆ˜ ëª¨ë¸"""
        market_size_score: int = Field(description="ì‹œì¥ ê·œëª¨ ì ìˆ˜ (0-25)")
        growth_score: int = Field(description="ì„±ì¥ì„± ì ìˆ˜ (0-30)")
        competition_score: int = Field(description="ê²½ìŸ í™˜ê²½ ì ìˆ˜ (0-25, ë‚®ì€ ê²½ìŸ=ë†’ì€ ì ìˆ˜)")
        risk_score: int = Field(description="ë¦¬ìŠ¤í¬ ì ìˆ˜ (0-20, ë‚®ì€ ë¦¬ìŠ¤í¬=ë†’ì€ ì ìˆ˜)")
        total_score: int = Field(description="ì´ì  (0-100)")
        justification: str = Field(description="ì ìˆ˜ ì‚°ì • ê·¼ê±°")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì‹œì¥ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ì¸ ì‹œì¥ì„± ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ì„¸ìš”:
- ì‹œì¥ ê·œëª¨ ì ìˆ˜: 0-25ì  (ê·œëª¨ê°€ í´ìˆ˜ë¡ ë†’ìŒ)
- ì„±ì¥ì„± ì ìˆ˜: 0-30ì  (ì„±ì¥ë¥ ì´ ë†’ì„ìˆ˜ë¡ ë†’ìŒ)
- ê²½ìŸ í™˜ê²½ ì ìˆ˜: 0-25ì  (ê²½ìŸì´ ì•½í• ìˆ˜ë¡ ë†’ìŒ)
- ë¦¬ìŠ¤í¬ ì ìˆ˜: 0-20ì  (ë¦¬ìŠ¤í¬ê°€ ë‚®ì„ìˆ˜ë¡ ë†’ìŒ)
í•©ê³„: 0-100ì """),
        ("human", """
ì‹œì¥: {query}

ì‹œì¥ ê·œëª¨: {market_size}
ì„±ì¥ ì¶”ì„¸: {growth_trend}
ê²½ìŸ í™˜ê²½: {competition}
ë¦¬ìŠ¤í¬ ìš”ì¸: {risk_factors}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ì„¸ìš”.""")
    ])
    
    structured_llm = llm.with_structured_output(MarketScore)
    score_result = structured_llm.invoke(prompt.format_messages(
        query=state["query"],
        market_size=state.get("market_size", "N/A"),
        growth_trend=state.get("growth_trend", "N/A"),
        competition=state.get("competition", "N/A"),
        risk_factors=state.get("risk_factors", "N/A")
    ))
    
    return {
        "final_score": score_result.total_score,
        "messages": [AIMessage(content=f"ì‹œì¥ì„± ì ìˆ˜ ì‚°ì¶œ ì™„ë£Œ: {score_result.total_score}/100ì \n\n{score_result.justification}")]
    }

def generate_final_report_node(state: MarketAnalysisState) -> Dict[str, Any]:
    """
    Step 6: ìµœì¢… ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    """
    prompt = PromptTemplate(
        input_variables=["query", "market_size", "growth_trend", "competition", 
                        "risk_factors", "final_score", "analysis_depth"],
        template="""ì¢…í•© ì‹œì¥ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ì‹œì¥: {query}
ë¶„ì„ ê¹Šì´: {analysis_depth}
ìµœì¢… ì‹œì¥ì„± ì ìˆ˜: {final_score}/100

=== ë¶„ì„ ê²°ê³¼ ===

1. ì‹œì¥ ê·œëª¨ ë¶„ì„
{market_size}

2. ì„±ì¥ ì¶”ì„¸ ë¶„ì„
{growth_trend}

3. ê²½ìŸ í™˜ê²½ ë¶„ì„
{competition}

4. ë¦¬ìŠ¤í¬ ìš”ì¸ ë¶„ì„
{risk_factors}

=== ì¢…í•© ë¦¬í¬íŠ¸ ì‘ì„± ì§€ì¹¨ ===
ë‹¤ìŒ êµ¬ì¡°ë¡œ executive summary í˜•ì‹ì˜ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

1. í•µì‹¬ ìš”ì•½ (2-3ë¬¸ë‹¨)
2. ì‹œì¥ ê¸°íšŒ í‰ê°€
3. ì£¼ìš” ì„±ê³µ ìš”ì¸
4. ê¶Œê³ ì‚¬í•­ (ì§„ì… ì „ëµ, ì£¼ì˜ì‚¬í•­)
5. ê²°ë¡ 

ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”."""
    )
    
    response = llm_creative.invoke([HumanMessage(
        content=prompt.format(
            query=state["query"],
            market_size=state.get("market_size", "N/A"),
            growth_trend=state.get("growth_trend", "N/A"),
            competition=state.get("competition", "N/A"),
            risk_factors=state.get("risk_factors", "N/A"),
            final_score=state.get("final_score", "N/A"),
            analysis_depth=state.get("analysis_depth", "intermediate")
        )
    )])
    
    return {
        "final_report": response.content,
        "messages": [AIMessage(content="ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")]
    }

# =============================================================================
# 6. ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜
# =============================================================================

def should_do_web_search(state: MarketAnalysisState) -> Literal["web_search", "skip_web_search"]:
    """ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
    if state.get("needs_web_search", False):
        return "web_search"
    return "skip_web_search"

def route_by_analysis_depth(state: MarketAnalysisState) -> Literal["detailed_analysis", "basic_analysis"]:
    """ë¶„ì„ ê¹Šì´ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
    depth = state.get("analysis_depth", "intermediate")
    if depth == "advanced":
        return "detailed_analysis"
    return "basic_analysis"

# =============================================================================
# 7. LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
# =============================================================================

def create_market_analysis_graph():
    """ì‹œì¥ì„± í‰ê°€ ê·¸ë˜í”„ ìƒì„±"""
    workflow = StateGraph(MarketAnalysisState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("classify_query", classify_query_node)
    workflow.add_node("retrieve_internal", retrieve_internal_data_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("analyze_market_size", analyze_market_size_node)
    workflow.add_node("analyze_growth", analyze_growth_trend_node)
    workflow.add_node("analyze_competition", analyze_competition_node)
    workflow.add_node("analyze_risks", analyze_risk_factors_node)
    workflow.add_node("calculate_score", calculate_market_score_node)
    workflow.add_node("generate_report", generate_final_report_node)
    
    # ì—£ì§€ êµ¬ì„±
    workflow.add_edge(START, "classify_query")
    workflow.add_edge("classify_query", "retrieve_internal")
    
    # ì¡°ê±´ë¶€ ì—£ì§€: ì›¹ ê²€ìƒ‰ ì—¬ë¶€
    workflow.add_conditional_edges(
        "retrieve_internal",
        should_do_web_search,
        {
            "web_search": "web_search",
            "skip_web_search": "analyze_market_size"
        }
    )
    
    workflow.add_edge("web_search", "analyze_market_size")
    
    # ìˆœì°¨ì  ë¶„ì„ ë‹¨ê³„
    workflow.add_edge("analyze_market_size", "analyze_growth")
    workflow.add_edge("analyze_growth", "analyze_competition")
    workflow.add_edge("analyze_competition", "analyze_risks")
    workflow.add_edge("analyze_risks", "calculate_score")
    workflow.add_edge("calculate_score", "generate_report")
    workflow.add_edge("generate_report", END)
    
    return workflow.compile()

# =============================================================================
# 8. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# =============================================================================

def run_market_analysis(query: str, verbose: bool = True):
    """ì‹œì¥ì„± í‰ê°€ ì‹¤í–‰"""
    graph = create_market_analysis_graph()
    
    initial_state = {
        "messages": [HumanMessage(content=query)],
        "query": query,
        "query_type": "",
        "rag_data": "",
        "web_data": "",
        "market_size": "",
        "growth_trend": "",
        "competition": "",
        "risk_factors": "",
        "final_score": 0,
        "final_report": "",
        "needs_web_search": False,
        "analysis_depth": "intermediate"
    }
    
    result = graph.invoke(initial_state)
    
    if verbose:
        print("\n" + "="*60)
        print(f"ğŸ“Œ ì‹œì¥ ë¶„ì„ ìš”ì²­: {query}")
        print("="*60)
        print(f"\n[ğŸ“Š] ìµœì¢… ì‹œì¥ì„± ì ìˆ˜: \033[1m{result['final_score']}/100\033[0m")
        print("\n[ğŸ“] ì¢…í•© ë¦¬í¬íŠ¸\n" + "-"*40)
        print(result['final_report'])
        print("-"*40)
        print("\n[ì „ì²´ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ í‘œì‹œ]")
        print(json.dumps({k: v for k, v in result.items() if k not in ("messages",)}, ensure_ascii=False, indent=2))
        print("="*60 + "\n")
    
    return result

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì˜ˆì‹œ 1: ê¸°ë³¸ ì‹œì¥ ë¶„ì„
    result1 = run_market_analysis("ì—ë“€í…Œí¬ ì‹œì¥ ë¶„ì„í•´ì¤˜")

    # ë³´ê¸° ì¢‹ê²Œ ìš”ì•½ ì¶œë ¥
    print("\n\n[ğŸ” ì—ë“€í…Œí¬ ì‹œì¥ ë¶„ì„ ìš”ì•½]")
    print(f"ìµœì¢… ì ìˆ˜: {result1['final_score']}/100")
    print("\n<< ìš”ì•½ë³¸ >>\n")
    print(result1['final_report'])
    print("\n[JSON ê²°ê³¼]\n")
    print(json.dumps({k: v for k, v in result1.items() if k not in ("messages",)}, ensure_ascii=False, indent=2))


    # ì˜ˆì‹œ 2: ê²½ìŸ í™˜ê²½ ì¤‘ì‹¬ ë¶„ì„
    # result2 = run_market_analysis("AI ì±—ë´‡ ì‹œì¥ì˜ ê²½ìŸ êµ¬ë„ëŠ”?")
    
    # ì˜ˆì‹œ 3: ì„±ì¥ ì „ë§ ì¤‘ì‹¬ ë¶„ì„
    # result3 = run_market_analysis("ì „ê¸°ì°¨ ë°°í„°ë¦¬ ì‹œì¥ 5ë…„ í›„ ì „ë§")